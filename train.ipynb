{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1: Train a Sentiment Analysis Classifier\n",
    "In this project, I have trained a sentiment analysis classifier for movie reviews. The sample code below builds a simple classifier that uses tf-idf to vectorize text and a logistic regression model to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the appropriate libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and take a quick look\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string, nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the functions that are responsible for the cleaning\n",
    "\n",
    "in the first step, I have created two functions. The first one will take the data that we have and will apply some preprocessing techniques to clean it. basically, in the first line, I am creating a copy of the data where always when we have data we need to create copy and work on the copy one to preserve the original data from being corrupted. after that, I have defined the patterns I am looking for for preprocessing the data. some words have upper case so by using .strip and lower() function i am converting the line to a lower case one. After that, some of them have opostrophes that needs to be cleaned as well that's when i use the handle function that is being created later that handles all the words that have opostrophes into regular words ex: shouldn't to should not. After that, I have to do two things which is removing the punctuation that can be done using string.punctuation and the stopwrods.\n",
    "\n",
    "the second function which is straightforwd i am creating the lemmatization function and returns the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning text and preprocess data\n",
    "def preprocessor(data):\n",
    "    \n",
    "    #creating a copy of the data to preserve the original data\n",
    "    data_copy = data\n",
    "    \n",
    "    #the pattern we are looking for\n",
    "    pattern1 = '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});'\n",
    "    pattern2 = r'[^a-zA-Z\\s]'\n",
    "    \n",
    "    #processing every line using the pattern set\n",
    "    data_copy = [re.sub(re.compile(pattern1), '', new_line) for new_line in data_copy]\n",
    "    data_copy = [re.sub(pattern2, ' ', new_line) for new_line in data_copy]\n",
    "    \n",
    "    #stripping white space and also after that converting the words into lowe case\n",
    "    lower_case_conversion = [new_line.strip().lower() for new_line in data_copy]\n",
    "    \n",
    "    # in here replaceing apostrophes that we have with words\n",
    "    processing = []\n",
    "    for new_line in lower_case_conversion:\n",
    "        new_line = new_line.replace(\"-\", \" \")\n",
    "        handled_word = [handle[new_word] if new_word in handle else new_word for new_word in new_line.split()]\n",
    "        processing.append(\" \".join(handled_word))\n",
    "        \n",
    "    #also punctuations needs to be removed so this line will do so\n",
    "    processing = [new_line.translate(str.maketrans('', '', string.punctuation)) for new_line in processing] \n",
    "    \n",
    "    # finally, stopwrods are being removed using the join function\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    processing = [\" \".join([new_word for new_word in new_line.split() if new_word not in stops]) for new_line in processing]\n",
    "    \n",
    "    #returning the fully processed text\n",
    "    return processing\n",
    "\n",
    "def lemmatize(data):\n",
    "    \n",
    "    #creating copy of the data\n",
    "    data_copy = data\n",
    "    \n",
    "    # lemmatization process\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    processed_data = [\" \".join([lemmatizer.lemmatize(new_word) for new_word in new_line.split()]) for new_line in data_copy]\n",
    "    \n",
    "    #return the processed lemmitization data\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the handle function\n",
    "\n",
    "as stated before, there are some words with opostrophes ( hasen't,aren't) these words needs to be handled so I have created a variable for it which contains almost all, if I did not forget any, words that has opostrophes that needs to be handled ex: he'd to he would"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some words needs to be handled like: hasn't to has not so this is a full list of all the word of the same form that needs to be\n",
    "#handled\n",
    "handle = {\n",
    "\"hasn\\'t\" : \"has not\",\n",
    "\"he\\'s\" : \"he is\",\n",
    "\"mightn\\'t\" : \"might not\",\n",
    "\"aren\\'t\" : \"are not\",\n",
    "\"can\\'t\" : \"cannot\",\n",
    "\"couldn\\'t\" : \"could not\",\n",
    "\"didn\\'t\" : \"did not\",\n",
    "\"haven\\'t\" : \"have not\",\n",
    "\"he\\'d\" : \"he would\",\n",
    "\"he\\'ll\" : \"he will\",\n",
    "\"doesn\\'t\" : \"does not\",\n",
    "\"don\\'t\" : \"do not\",\n",
    "\"hadn\\'t\" : \"had not\",\n",
    "\"i\\'d\" : \"I would\",\n",
    "\"i\\'d\" : \"I had\",\n",
    "\"i\\'ll\" : \"I will\",\n",
    "\"mustn\\'t\" : \"must not\",\n",
    "\"shan\\'t\" : \"shall not\",\n",
    "\"she\\'d\" : \"she would\",\n",
    "\"she\\'ll\" : \"she will\",\n",
    "\"i\\'m\" : \"I am\",\n",
    "\"isn\\'t\" : \"is not\",\n",
    "\"it\\'s\" : \"it is\",\n",
    "\"let\\'s\" : \"let us\",\n",
    "\"she\\'s\" : \"she is\",\n",
    "\"shouldn\\'t\" : \"should not\",\n",
    "\"that\\'s\" : \"that is\",\n",
    "\"we\\'d\" : \"we would\",\n",
    "\"we\\'re\" : \"we are\",\n",
    "\"weren\\'t\" : \"were not\",\n",
    "\"we\\'ve\" : \"we have\",\n",
    "\"there\\'s\" : \"there is\",\n",
    "\"they\\'d\" : \"they would\",\n",
    "\"it\\'ll\":\"it will\",\n",
    "\"i\\'ve\" : \"I have\",\n",
    "\"\\'re\": \" are\",\n",
    "\"wasn\\'t\": \"was not\",\n",
    "\"we\\'ll\":\" will\",\n",
    "\"they\\'ll\" : \"they will\",\n",
    "\"they\\'re\" : \"they are\",\n",
    "\"they\\'ve\" : \"they have\",\n",
    "\"what\\'ll\" : \"what will\",\n",
    "\"what\\'re\" : \"what are\",\n",
    "\"who\\'d\" : \"who would\",\n",
    "\"who\\'ll\" : \"who will\",\n",
    "\"who\\'re\" : \"who are\",\n",
    "\"who\\'s\" : \"who is\",\n",
    "\"who\\'ve\" : \"who have\",\n",
    "\"what\\'s\" : \"what is\",\n",
    "\"what\\'ve\" : \"what have\",\n",
    "\"where\\'s\" : \"where is\",\n",
    "\"won\\'t\" : \"will not\",\n",
    "\"wouldn\\'t\" : \"would not\",\n",
    "\"you\\'d\" : \"you would\",\n",
    "\"you\\'ll\" : \"you will\",\n",
    "\"you\\'re\" : \"you are\",\n",
    "\"you\\'ve\" : \"you have\",\n",
    "\"didn\\'t\": \"did not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Enjoy the opening credits. They're the best th...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Well, the Sci-Fi channel keeps churning these ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>It takes guts to make a movie on Gandhi in Ind...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Nest is really just another 'nature run am...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Waco: Rules of Engagement does a very good job...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text sentiment\n",
       "0           0  Enjoy the opening credits. They're the best th...       neg\n",
       "1           1  Well, the Sci-Fi channel keeps churning these ...       neg\n",
       "2           2  It takes guts to make a movie on Gandhi in Ind...       pos\n",
       "3           3  The Nest is really just another 'nature run am...       neg\n",
       "4           4  Waco: Rules of Engagement does a very good job...       pos"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the data from the datafile and displaying the first 5 rows\n",
    "raw = pd.read_csv('coursework1_train.csv')\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning table\n",
    "\n",
    "as we can see when we called the table, there is a column that has unnamed: 0 and another that is sentiment. sentiment is very important because this is the column that will run our learning model on but the other one needs to be deleted so in the first line i am trying to get rid of null values then I have deleted the unnamed: 0 column with the del functionality then the third one the sentiment because its not in a binary form ( number form ) its pos/neg i have transformed it into numbers where positive is 1 and negative is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in here i am trying to check if there are any null data\n",
    "raw.isnull().sum()\n",
    "\n",
    "#deleting unnamed 0 if any \n",
    "if('Unnamed: 0' in raw.columns):\n",
    "    del raw['Unnamed: 0']\n",
    "\n",
    "raw.sentiment.value_counts()\n",
    "\n",
    "#renaming the values of the sentiment from pos/neg to 1/0\n",
    "raw['sentiment'] = raw.sentiment.map(lambda x: 1 if x =='pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry num 40000\n",
      "num of pos entries 20000\n",
      "num of neg entries 20000\n"
     ]
    }
   ],
   "source": [
    "# check the size of the data and its class distribution\n",
    "all_text = raw['text'].tolist()\n",
    "all_lables = raw['sentiment'].tolist()\n",
    "\n",
    "print('entry num', len(all_text))\n",
    "print('num of pos entries', len([l for l in all_lables if l==1]))\n",
    "print('num of neg entries', len([l for l in all_lables if l==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data\n",
    "\n",
    "in this step, its just giving the data ( all_text ) to the two functions that I have created earlier for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now using the preprocessing function created with the lemmatization\n",
    "data_processed = preprocessor(all_text)\n",
    "data_processed = lemmatize(data_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Returning values\n",
    "\n",
    "after applying the functions, I have returned the values to the original data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the processed data to the raw data as text\n",
    "raw['text'] = data_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data\n",
    "\n",
    "pretty straightforwd plitting the data into testing and training based on the text and the sentiment where the test size will be 25% of the data and the training data is 75%, and the random state is dd/mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into training and testing while setting the random state as dd/mm\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw['text'], raw['sentiment'], test_size=0.25, random_state=408)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf initialization\n",
    "train_vectorizer = TfidfVectorizer(max_features=5000,use_idf=True)\n",
    "train_vecs = train_vectorizer.fit_transform(X_train)\n",
    "test_vecs = TfidfVectorizer(max_features=5000,vocabulary=train_vectorizer.vocabulary_).fit_transform(X_test)\n",
    "\n",
    "tfidf_feature_names = train_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(max_features=5000)\n",
    "X_train_vectorized = count_vect.fit_transform(X_train)\n",
    "X_test_vectorized = CountVectorizer(max_features=5000,vocabulary=count_vect.vocabulary_).fit_transform(X_test)\n",
    "\n",
    "cv_feature_names = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "after creating and initializing the model, we need to tune it using couple of tuning techniques and params so this is where this step comes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in here, creating the log hyperparameters dictionary with the penalty which is l1 l2 then creating the log classifier with\n",
    "#gridsearchvs\n",
    "log_hyperparameters = dict(C=[0.001, 0.01, 0.1, 1, 10, 100, 1000], penalty=['l1', 'l2'])\n",
    "log_clf = GridSearchCV(LogisticRegression(), log_hyperparameters, cv=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "First, I have chose LogicsticRegression as my model and I fit the vectorized data with the y_train in the logistics regression model. After that, I am choosing the best C and best penalty for the model. when this steps are done, I have created another classifier with the corresponding C and penalty to fit the data again and after that I have predicted the model accuracy according to it. The final thing I did was creating the confusion matrix and getting a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Penalty to choose : l2\n",
      "the C to choose for battle: 1\n",
      "the accuracy 0.8833\n",
      "[[4368  624]\n",
      " [ 543 4465]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88      4992\n",
      "           1       0.88      0.89      0.88      5008\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training: tf-idf + logistic regression\n",
    "# train model\n",
    "\n",
    "model = log_clf.fit(train_vecs, y_train)\n",
    "penalty = model.best_estimator_.get_params()['penalty']\n",
    "c = model.best_estimator_.get_params()['C']\n",
    "print('the Penalty to choose :', penalty)\n",
    "print('the C to choose for battle:', c)\n",
    "\n",
    "clf = LogisticRegression(C=c,penalty=penalty).fit(train_vecs, y_train)\n",
    "\n",
    "# test model\n",
    "prediction_test_value = clf.predict(test_vecs)\n",
    "\n",
    "acc = accuracy_score(y_test, prediction_test_value)\n",
    "print('the accuracy', acc)\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, prediction_test_value))\n",
    "\n",
    "# classification_report\n",
    "print(classification_report(y_test, prediction_test_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the weight classification\n",
    "\n",
    "in this step, I have classified some weights based on how well they do like worst,waste,perfect... so I have used the enumerate to get the weights then sort them using the sort functionality and return them in a dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worst</th>\n",
       "      <th>waste</th>\n",
       "      <th>bad</th>\n",
       "      <th>awful</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "      <th>boring</th>\n",
       "      <th>terrible</th>\n",
       "      <th>poor</th>\n",
       "      <th>perfect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.479449</td>\n",
       "      <td>7.38444</td>\n",
       "      <td>7.332046</td>\n",
       "      <td>6.988097</td>\n",
       "      <td>6.753663</td>\n",
       "      <td>6.585347</td>\n",
       "      <td>6.350889</td>\n",
       "      <td>5.297323</td>\n",
       "      <td>5.229483</td>\n",
       "      <td>5.104207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      worst    waste       bad     awful  excellent     great    boring  \\\n",
       "0  9.479449  7.38444  7.332046  6.988097   6.753663  6.585347  6.350889   \n",
       "\n",
       "   terrible      poor   perfect  \n",
       "0  5.297323  5.229483  5.104207  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = {}\n",
    "for (index,weight) in enumerate(tfidf_feature_names):\n",
    "    weights[weight] = abs(clf.coef_[0][index])\n",
    "\n",
    "sorted_weights = {key: value for key, value in sorted(weights.items(), key=lambda item: item[1], reverse = True)}\n",
    "\n",
    "df = pd.DataFrame([sorted_weights])\n",
    "df.iloc[:,0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating another model\n",
    "\n",
    "in here, I am creating another one using countvectorizer with logistic regression where before it was tf-id with logistic regression. same steps applies with the steps and everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best penalty: l2\n",
      "the best C to choose for battle: 1\n",
      "the accuracy is: 0.8655\n",
      "[[4309  683]\n",
      " [ 662 4346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.87      4992\n",
      "           1       0.86      0.87      0.87      5008\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training: CountVectorizer + logistic regression\n",
    "# train model\n",
    "\n",
    "model = log_clf.fit(train_vecs, y_train)\n",
    "penalty = model.best_estimator_.get_params()['penalty']\n",
    "c = model.best_estimator_.get_params()['C']\n",
    "print('the best penalty:', penalty)\n",
    "print('the best C to choose for battle:', c)\n",
    "\n",
    "clf = LogisticRegression(C=c,penalty=penalty).fit(X_train_vectorized, y_train)\n",
    "\n",
    "# test model\n",
    "prediction_test_value = clf.predict(X_test_vectorized)\n",
    "\n",
    "acc = accuracy_score(y_test, prediction_test_value)\n",
    "print('the accuracy is:', acc)\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, prediction_test_value))\n",
    "\n",
    "# classification_report\n",
    "print(classification_report(y_test, prediction_test_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>waste</th>\n",
       "      <th>forgettable</th>\n",
       "      <th>fails</th>\n",
       "      <th>refreshing</th>\n",
       "      <th>sensitive</th>\n",
       "      <th>worst</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>notch</th>\n",
       "      <th>mildly</th>\n",
       "      <th>unwatchable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.354888</td>\n",
       "      <td>2.232165</td>\n",
       "      <td>2.147537</td>\n",
       "      <td>2.03489</td>\n",
       "      <td>2.008938</td>\n",
       "      <td>1.997186</td>\n",
       "      <td>1.981454</td>\n",
       "      <td>1.977147</td>\n",
       "      <td>1.965383</td>\n",
       "      <td>1.961699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      waste  forgettable     fails  refreshing  sensitive     worst  \\\n",
       "0  2.354888     2.232165  2.147537     2.03489   2.008938  1.997186   \n",
       "\n",
       "   disappointment     notch    mildly  unwatchable  \n",
       "0        1.981454  1.977147  1.965383     1.961699  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = {}\n",
    "for (index,weight) in enumerate(cv_feature_names):\n",
    "    weights[weight] = abs(clf.coef_[0][index])\n",
    "\n",
    "sorted_weights = {key: value for key, value in sorted(weights.items(), key=lambda item: item[1], reverse = True)}\n",
    "\n",
    "df = pd.DataFrame([sorted_weights])\n",
    "df.iloc[:,0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes with tf-idf\n",
    "\n",
    "as the title suggest, I have create another classifier with Naive bayes and tf-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy is: 0.8519\n",
      "[[4232  760]\n",
      " [ 721 4287]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      4992\n",
      "           1       0.85      0.86      0.85      5008\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training: tf-idf + Naive Bayes\n",
    "# train model\n",
    "clf = naive_bayes.MultinomialNB()\n",
    "clf.fit(train_vecs, y_train)\n",
    "\n",
    "# test model\n",
    "prediction_test_value = clf.predict(test_vecs)\n",
    "acc = accuracy_score(y_test, prediction_test_value)\n",
    "print('the accuracy is:', acc)\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, prediction_test_value))\n",
    "\n",
    "# classification_report\n",
    "print(classification_report(y_test, prediction_test_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer with Naive Bayes\n",
    "\n",
    "in here, I have used CountVectorizer with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy is: 0.8441\n",
      "[[4237  755]\n",
      " [ 804 4204]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84      4992\n",
      "           1       0.85      0.84      0.84      5008\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training: CountVectorizer + Naive Bayes\n",
    "# train model\n",
    "\n",
    "clf = naive_bayes.MultinomialNB()\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# test model\n",
    "prediction_test_value = clf.predict(X_test_vectorized)\n",
    "acc = accuracy_score(y_test, prediction_test_value)\n",
    "print('the accuracy is:', acc)\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, prediction_test_value))\n",
    "\n",
    "# classification_report\n",
    "print(classification_report(y_test, prediction_test_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE YOUR TRAINED MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save model and other necessary modules\n",
    "all_info_want_to_save = {\n",
    "    'model': clf,\n",
    "    'vectorizer': TfidfVectorizer(max_features=5000,vocabulary=train_vectorizer.vocabulary_),\n",
    "    'negate_handle' : handle,\n",
    "    'feature_names' : tfidf_feature_names\n",
    "}\n",
    "with open(\"sample_trained_model.pickle\", \"wb\") as f:\n",
    "    pickle.dump(all_info_want_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
